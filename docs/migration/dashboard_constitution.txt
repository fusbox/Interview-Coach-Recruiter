Interview Coach App
Data Logging vs Inference Map
(with Signal Quality, Confidence Weighting, Decision Table & QA Test Matrix)
________________________________________
Purpose of This Document
This document defines what data is logged, what data is derived, and what is inferred at runtime for the Interview Coach app. It also establishes signal quality thresholds, confidence-weighted delivery rules, and testable decision logic for dashboard behavior.
This is a load bearing specification. Violations of these rules will degrade trust, interpretability, and long term scalability.
________________________________________
1. Core Principle
Log facts. Infer meaning. Never store conclusions.
Persist only stable, auditable facts. Compute metrics from those facts. Infer meaning only at render time.
If conclusions are stored, the system freezes yesterday’s interpretation and cannot improve without data corruption.
________________________________________
2. Three Layer Data Architecture (Non Negotiable)
Layer 1 — Raw Session Facts (Persisted)
What objectively happened
Layer 2 — Derived Metrics (Computed, optionally cached)
What patterns can be calculated from facts
Layer 3 — Interpretive Signals (Ephemeral)
What the system currently believes this means
Only Layer 1 is permanently stored.
________________________________________
3. Layer 1 — Raw Session Facts (Persisted)
3.1 Session Metadata
•	Session ID
•	User ID / Member ID
•	Target role ID
•	Session start / end timestamps
•	Session type (practice, assessment, recruiter guided)
•	Question set ID + version
Why: Enables auditability, reprocessing, and future model evolution.
________________________________________
3.2 Question Interaction Facts (Per Question)
•	Question ID
•	Question competency tags (design time)
•	Response duration
•	Completion / abandonment flag
•	Retry count
Why: Time and completion are objective; tags are external truth, not judgment.
________________________________________
3.3 Structural Markers (Detected, Not Judged)
•	STAR components detected (S / T / A / R flags)
•	Explicit outcome statement detected
•	Concrete example markers detected
•	Reflection / learning markers detected
Why: Observable features only; no quality inference.
________________________________________
3.4 User Declared Inputs
•	Self reported confidence (global and/or per competency)
•	Pre session intent flags
Why: Subjective input must be preserved verbatim.
________________________________________
3.5 Explicitly Forbidden in Layer 1
•	Scores or ratings
•	Readiness labels
•	Coaching recommendations
•	Tone or sentiment judgments
•	Percentiles or rankings
Reason: These are interpretations, not facts.
________________________________________
4. Layer 2 — Derived Metrics (Computed)
Derived metrics may be cached but must be recomputable from Layer 1.
4.1 Competency Aggregates
•	STAR completion rate per competency
•	Outcome inclusion rate
•	Specificity proxy (concrete markers / response length)
•	Relevance proxy (tag match coverage)
________________________________________
4.2 Consistency & Variance
•	Intra session variance per competency
•	Inter session variance (rolling window)
•	Completion stability
Note: Stability often matters more than averages.
________________________________________
4.3 Alignment Metrics
•	Confidence vs observed behavior delta
•	Direction of mismatch (no labeling)
________________________________________
4.4 What Not to Persist
•	Final scores
•	Threshold flags
•	Percent improvement values
Reason: Locks interpretation and invites false certainty.
________________________________________
5. Layer 3 — Interpretive Signals (Never Persisted)
Layer 3 signals are recomputed at render time and drive dashboard content.
5.1 Section 1 — Current Baseline
Derived from:
•	Recent competency aggregates
•	Consistency patterns
•	Confidence alignment
•	Role weighting
Output:
•	Single qualitative state descriptor
•	Confidence adjusted tone
•	Temporal grounding
________________________________________
5.2 Section 2 — Competency Constellation
Derived from:
•	Relative competency strengths
•	Confidence alignment
•	Role relevance weighting
Output:
•	Shape, not judgment
________________________________________
5.3 Section 3 — Coaching Focus
Derived from:
•	Role critical gaps
•	Repeated patterns
•	Coachability heuristics
•	Psychological readiness
Output:
•	One focus
•	One rationale
•	One next action
Never persist recommendations.
________________________________________
5.4 Section 4 — Progress & Momentum
Derived from:
•	Session to session deltas
•	Variance trends
•	Focus follow through
Output:
•	Directional signals only
________________________________________
6. Forbidden Data (Anywhere)
•	User to user comparisons
•	Pass / fail labels
•	Lifetime averages surfaced to users
•	Model confidence values
________________________________________
7. Signal Quality Rules
7.1 Signal Quality Factors
1.	Sample size (sessions / answers)
2.	Consistency (variance vs stability)
3.	Recency (decay of old data)
4.	Role relevance
________________________________________
7.2 Signal Quality Levels (Internal)
•	Insufficient
•	Emerging
•	Reliable
•	Strong
These labels are never shown to users.
________________________________________
7.3 Signal Quality Effects by Dashboard Section
•	Baseline: Tone softens with weak signal
•	Constellation: Visual compression under low signal
•	Coaching Focus: Foundational guidance if signal is weak
•	Progress: No trend claims without sufficient evidence
________________________________________
8. Confidence Weighting Rules
8.1 Core Rule
Confidence affects delivery, not truth.
________________________________________
8.2 Delivery Modulation
•	Low confidence → stabilizing, supportive
•	Medium confidence → neutral, directive
•	High confidence → precise, corrective
________________________________________
8.3 Signal × Confidence Interaction Matrix
Signal Quality	User Confidence	Delivery Style
Low	Low	Gentle, exploratory
Low	High	Curious, provisional
High	Low	Supportive, reassuring
High	High	Direct, corrective
________________________________________
9. Decision Table — Coaching Focus Selection
Condition	Rule
Role critical weakness + strong signal	Select as focus
Repeated mild weakness + emerging signal	Select as exploratory focus
Single severe weakness + weak signal	Defer; choose foundational skill
High variance across competencies	Focus on consistency
Low confidence + weak signal	Choose stabilizing, general skill
________________________________________
10. QA Test Matrix (Behavioral)
10.1 Baseline Rendering
Scenario	Expected Behavior
1 session only	Tentative language, no strong claims
3 consistent sessions	Clear baseline summary
High variance	Language emphasizes inconsistency
________________________________________
10.2 Coaching Focus Stability
Scenario	Expected Behavior
Same weakness across 3 sessions	Focus persists
One off anomaly	Focus does not flip
Role change	Focus re weighted
________________________________________
10.3 Progress & Momentum
Scenario	Expected Behavior
Small steady gains	Momentum highlighted
Flat but stable	Plateau normalized
Volatile swings	Framed as uneven
________________________________________
Closing Note
This specification encodes epistemic humility into product behavior. The system speaks only when evidence earns the right—and speaks differently depending on who is listening.
This is how the Interview Coach becomes trustworthy at scale.
________________________________________
11. Telemetry Schema / Event Contract (Derived)
This section translates the conceptual rules above into a minimal, enforceable telemetry contract. The goal is consistency, auditability, and future re-interpretation.
11.1 Event Envelope (All Events)
Every event must include:
•	event_id
•	event_name
•	event_timestamp
•	user_id / member_id
•	session_id
•	role_id (if applicable)
•	source_surface (dashboard, interview, job-search, resume)
Rule: No event may imply quality, success, or readiness.
________________________________________
11.2 Tier 1 — Presence Events
Event Name	Required Fields
app_foregrounded	visibility_state
app_backgrounded	visibility_state
page_visible	page_id
page_hidden	page_id
session_start	session_type
session_end	reason
Invariant: Presence events never accrue time.
________________________________________
11.3 Tier 2 — Interaction Events
Event Name	Required Fields	Notes
scroll_depth_change	page_id, depth	Debounced
navigation_click	from_page, to_page	
typing_activity	field_id	Debounced
search_action	query_type	
media_control	action_type	play / pause / seek
form_interaction	form_id	
Invariant: Interaction sustains windows but does not accrue time alone.
________________________________________
11.4 Tier 3 — Task Engagement Events
Interview Coach
•	interview_question_started
•	interview_response_started
•	interview_response_completed
•	interview_response_revised
•	coaching_action_completed
Job-Seeking
•	job_search_executed
•	job_listing_viewed
•	job_application_started
•	job_application_step_completed
•	job_application_submitted
Resume / Profile
•	resume_upload
•	resume_edit_started
•	resume_edit_completed
•	profile_update_completed
Passive Content (Conditional)
•	content_view_started
•	content_view_progress
•	content_view_completed
Invariant: Only Tier 3 events open engagement windows.
________________________________________
11.5 Engagement Window State (Derived, Not Stored)
Window attributes (runtime only):
•	window_start
•	last_interaction_timestamp
•	task_category
•	active_weight
Rule: Window closes immediately on presence loss or inactivity timeout.
________________________________________
12. Trust Regression Checklist (Product & QA Guardrail)
This checklist must be reviewed for every dashboard or analytics change.
12.1 Evidence Integrity
•	Are we logging only observable facts?
•	Can this data be reinterpreted in the future?
•	Are any conclusions being stored?
________________________________________
12.2 Signal Quality Discipline
•	Does this feature speak only when evidence is sufficient?
•	Are we suppressing claims under high variance?
•	Do new visuals exaggerate weak signals?
________________________________________
12.3 Confidence-Aware Delivery
•	Does tone adapt to user confidence without changing truth?
•	Are low-confidence users protected from overload?
•	Are high-confidence users challenged appropriately?
________________________________________
12.4 Anti-Gaming & Metrics Hygiene
•	Can idle behavior inflate metrics?
•	Can trivial repetition extend engagement windows?
•	Are caps and decay still intact?
________________________________________
12.5 Interpretability & Dignity
•	Can this be explained to a non-technical MCO stakeholder?
•	Would a user feel judged or ranked?
•	Does this preserve user agency?
________________________________________
12.6 Smell Tests (Immediate Stop Signals)
Stop and revisit design if any answer is yes:
•	“We added this because we had the data.”
•	“It looks impressive but we can’t explain it.”
•	“Users might optimize for this metric.”
•	“We’ll fix interpretation later.”
________________________________________
Closing Enforcement Principle
If a feature violates this checklist, it should not ship—regardless of how easy it is to build or how exciting it looks.
Trust, once lost, is not recoverable by iteration.
________________________________________
13. Golden Telemetry Examples (Canonical JSON Payloads)
These examples are reference implementations, not suggestions. Any deviation must be intentional.
13.1 Presence Event (Tier 1)
{
  "event_id": "evt-001",
  "event_name": "app_foregrounded",
  "event_timestamp": "2026-02-01T14:32:10Z",
  "user_id": "user-123",
  "session_id": "sess-456",
  "source_surface": "dashboard",
  "visibility_state": "foreground"
}
________________________________________
13.2 Interaction Event (Tier 2)
{
  "event_id": "evt-014",
  "event_name": "typing_activity",
  "event_timestamp": "2026-02-01T14:34:02Z",
  "user_id": "user-123",
  "session_id": "sess-456",
  "source_surface": "interview",
  "field_id": "interview_response_text"
}
________________________________________
13.3 Task Engagement Event (Tier 3)
{
  "event_id": "evt-027",
  "event_name": "interview_response_completed",
  "event_timestamp": "2026-02-01T14:38:45Z",
  "user_id": "user-123",
  "session_id": "sess-456",
  "role_id": "role-software-engineer",
  "source_surface": "interview",
  "question_id": "q-009",
  "competency_tags": ["structure", "impact"]
}
________________________________________
14. Dashboard Language Style Guide (Signal-Aware Copy)
This guide ensures language tracks evidence strength and respects user confidence.
14.1 Baseline Language by Signal Quality
Signal Quality	Language Pattern
Insufficient	"Based on limited recent practice, it appears…"
Emerging	"Your recent answers suggest…"
Reliable	"Your answers are consistently…"
Strong	"You consistently demonstrate…"
________________________________________
14.2 Coaching Focus Language by Confidence Level
User Confidence	Language Tone
Low	Supportive, normalizing
Medium	Neutral, instructional
High	Direct, corrective
________________________________________
14.3 Prohibited Language (Anywhere)
•	"You failed"
•	"You are weak at"
•	"Most candidates do better"
•	"Your score is"
•	"You should already know"
________________________________________
15. Recruiter & MCO Read-Only Analytics Specification
This spec defines what can be exposed externally without contaminating the candidate experience.
15.1 Data Scope (Aggregate or Read-Only)
Allowed:
•	Verified Engagement Time (by activity category)
•	Session counts (non-gamified)
•	Task completion rates
•	Time-to-first-application
•	Coaching follow-through rates
Disallowed:
•	Individual coaching recommendations
•	Qualitative baseline descriptors
•	Confidence self-reports
•	Competency-level judgments
________________________________________
15.2 Privacy-Safe Aggregation Rules
•	Minimum cohort size enforced
•	No cross-member comparison surfaced
•	No percentile rankings
•	No individual-level trend extrapolation
________________________________________
15.3 Interpretive Guardrails for External Reporting
External dashboards may say:
•	"Members engaging in interview practice weekly show higher application completion rates."
They must not say:
•	"Interview coaching caused placement success."
Correlation may be shown. Causation may not be claimed.
________________________________________
Closing Architecture Reminder
The same raw facts power:
•	Candidate coaching
•	Recruiter insight
•	MCO business intelligence
Only interpretation layers differ.
This separation is intentional—and non-negotiable.
________________________________________
16. Dashboard Component Guidance (Desktop & Mobile)
This section defines canonical dashboard components, their desktop and mobile adaptations, and allowed states. These rules prevent silent erosion of trust through UI drift.
Component categories:
•	Sacred components encode epistemology and must not be reinterpreted.
•	Constrained components may vary visually but not behaviorally.
•	Utility components are reusable with minimal risk.
________________________________________
16.1 Sacred Components (High-Integrity)
A. CurrentBaselineBlock
Purpose: Orient the user with a synthesized, present-tense read.
Props:
•	baseline_text (string)
•	grounding_text (string)
Desktop behavior:
•	Prominent placement at top
•	Large, readable text
•	No icons, charts, or actions
Mobile behavior:
•	First visible content in safe area
•	Max 2 lines + grounding line
•	Line breaks intentional for breath
Hard constraints:
•	No numbers
•	No trends
•	No CTA
Why sacred: This is the system’s voice. If it becomes metric-driven, trust collapses.
________________________________________
B. CoachingFocusCard
Purpose: Convert insight into a single behavioral lever.
Props:
•	focus_statement
•	role_rationale
•	next_action
Desktop behavior:
•	Card treatment with visual emphasis
•	Clear hierarchy: focus → why → action
Mobile behavior:
•	Tall card, full-width
•	Optional sticky CTA below card (not inside)
Hard constraints:
•	Exactly one focus
•	Exactly one action
•	No expandable sections
Why sacred: This is the behavior-change engine. Dilution breaks outcomes.
________________________________________
16.2 Constrained Components (Medium Risk)
C. CompetencyConstellation
Purpose: Show relative shape of strengths and gaps without judgment.
Props:
•	competency_list[]
•	relative_strengths
•	confidence_alignment
Desktop variants (choose one):
•	Muted radar
•	Dot constellation
•	Horizontal bars
Mobile variant (preferred):
•	Vertical micro-bars (one row per competency)
Constraints (all platforms):
•	Relative only (no absolutes)
•	No rankings or totals
•	Confidence alignment via opacity/weight, not color
•	Hover/tap → single-sentence explanation only
________________________________________
D. ProgressMomentumIndicator
Purpose: Answer “is this worth continuing?” without pressure.
Props:
•	direction (up / flat / uneven)
•	short_description
Desktop behavior:
•	Small sparkline or arrow + text
Mobile behavior:
•	Text-first, optional tiny slope indicator
Constraints:
•	No axes
•	No percentages
•	No red/green semantics
________________________________________
16.3 Utility Components (Low Risk)
E. InsightTooltip
•	One sentence maximum
•	No advice, no metrics
•	Used only for clarification
F. SessionCTAButton
•	Neutral language (e.g., “Start a practice session”)
•	No urgency or streak framing
•	Detached from performance state
________________________________________
17. Component State Matrix
All components must respect signal quality and data availability. States are behavioral, not just visual.
17.1 Global States
State	Description
Loading	Data not yet available
Low Signal	Insufficient or emerging evidence
High Signal	Reliable or strong evidence
Empty	No sessions yet
________________________________________
17.2 CurrentBaselineBlock States
State	Expected Behavior
Loading	Neutral placeholder, no claims
Empty	Orientation copy only (“Practice to build your baseline”)
Low Signal	Tentative language (“appears…”)
High Signal	Clear, calm descriptor
________________________________________
17.3 CompetencyConstellation States
State	Expected Behavior
Loading	Skeleton UI, no shape implied
Empty	Hidden or replaced with brief explainer
Low Signal	Compressed differences, softened extremes
High Signal	Clearer relative contrast
________________________________________
17.4 CoachingFocusCard States
State	Expected Behavior
Loading	Hidden (never speculate)
Empty	Foundational skill suggestion
Low Signal	Exploratory focus (“try focusing on…”)
High Signal	Targeted corrective focus
________________________________________
17.5 ProgressMomentumIndicator States
State	Expected Behavior
Loading	Hidden
Empty	Hidden
Low Signal	No trend claims
High Signal	Directional language only
________________________________________
18. Enforcement Rule
If a component violates its state rules or constraints, it must not render.
Silence is preferable to confident nonsense.

